#!/bin/bash -l

#SBATCH -t 00:30:00
#SBATCH -N 1
#SBATCH -J ExaEpi
#SBATCH -A m5071_g
#SBATCH -q regular
#SBATCH -C gpu
#SBATCH --exclusive
#SBATCH --cpus-per-task=32
#SBATCH --gpu-bind=none
#SBATCH --ntasks-per-node=4
#SBATCH --gpus-per-node=4
#SBATCH -o ExaEpi.o%j
#SBATCH -e ExaEpi.e%j


OUTFILE=out.${NERSC_HOST}.log

INP=$(ls inputs*)
echo "Input file is ${INP}."

EXEC=$(ls $EXAEPI_BUILD/bin/agent)
echo "Executable file is ${EXEC}."

# pin to closest NIC to GPU
export MPICH_OFI_NIC_POLICY=GPU
export OMP_NUM_THREADS=1
GPU_AWARE_MPI="amrex.use_gpu_aware_mpi=1"

# CUDA visible devices are ordered inverse to local task IDs
#   Reference: nvidia-smi topo -m
rm -rf Backtrace* plt* cases* $OUTFILE output.dat *.core
srun --cpu-bind=cores bash -c "
    export CUDA_VISIBLE_DEVICES=\$((3-SLURM_LOCALID));
    compute-sanitizer --tool memcheck ${EXEC} ${INP} ${GPU_AWARE_MPI}" \
    2>&1 |tee $OUTFILE
